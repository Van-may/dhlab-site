---
<!--browsable: false-->
title: Machine Vision for Cultural Heritage & Natural Science Collections
image: /assets/images/projects/originals/yale-smithsonian-banner.jpg
thumbnail: /assets/images/projects/originals/yale-smithsonian-banner.jpg
alt: Collage of cultural heritage images, including photographs, paintings, and sculptures
<!--video: /assets/images/projects/originals/dance.mp4-->
project_url:
categories:
  - Text Analysis
  - Visual Analysis
tags:
  - Literature
  - Machine Learning
  - Neural Networks
  - Photographs
team:
  - name: Yale Digital Humanities Laboratory
  - name: Smithsonian Institution
permalink: '/machine-vision/'
---
<span style='color:#87AFC7'>*Data Ethics • Deep Learning • Perceptual Hashing • Convolutional Neural Networks • Semantic Segmentation • International Image Interoperability Framework • Neural Style Transfer • Data Science • Generative Adversarial Networks • Visualization • Variational Autoencoders • Manifold Learning • Transfer Learning • Computer-Assisted Curation • Computer Vision • Automated Captioning • Dimensionality Reduction • Culturally-Sensitive Data*</span>

### Overview

The mass digitization of visual collections, on the order of hundreds of thousands or millions of images, creates new challenges for curators and researchers alike. Simultaneously, the rapid pace of industry innovation in deep learning (from guiding self-driving cars to captioning smartphone images) demands the attention of library, museum, and academic professionals.

Existing practices of cataloging and description can be augmented by recent advancements in machine vision, and human expertise can likewise guide the development of future algorithms for the humanities and sciences alike. To showcase work that's underway and explore potential collaborations, the Yale-Smithsonian Partnership held a one-day symposium on the topic of machine learning for cultural heritage and natural science collections. The event brought together scholars and curators from both institutions for conversations, demonstrations, and future partnerships.

### Keynote Speakers
- [Rebecca Dikow](https://datascience.si.edu/people/dr-rebecca-dikow) ([Data Science Lab](https://datascience.si.edu), Smithsonian Institution)
- [Holly Rushmeier](http://graphics.cs.yale.edu/site/people/holly-rushmeier) (Computer Science, Yale)

### Logistics
- Date: April 26, 2019
- Time: 10:00 a.m. – 4:00 p.m.
- Location: Franke Family Digital Humanities Laboratory, Sterling Memorial Library, 120 High Street, New Haven, CT

### Schedule  
<table class="unchanged rich-diff-level-one">
  <thead>
    <tr>
      <th>Start</th>
      <th>End</th>
      <th>Name</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>9:30am</td>
      <td>10:00am</td>
      <td></td>
      <td>Registration &amp; Coffee</td>
    </tr>
    <tr>
      <td>10:00am</td>
      <td>10:05am</td>
      <td>Peter Leonard</td>
      <td>Welcome</td>
    </tr>
    <tr>
      <td>10:05am</td>
      <td>10:35am</td>
      <td>
      <a href="https://datascience.si.edu/people/dr-rebecca-dikow" rel="nofollow">Rebecca Dikow</a> (<a href="https://datascience.si.edu" rel="nofollow">Data Science Lab</a>, Smithsonian Institution)</td>
      <td><strong>Data-Intensive Approaches to Digitized Museum Collections</strong></td>
    </tr>
    <tr>
      <td>10:35am</td>
      <td>11:05am</td>
      <td>
      <a href="http://graphics.cs.yale.edu/site/people/holly-rushmeier" rel="nofollow">Holly Rushmeier</a> (Computer Science, Yale)</td>
      <td><strong>Collecting Useful Data</strong></td>
    </tr>
    <tr>
      <td>11:05am</td>
      <td>11:25am</td>
      <td><a href="http://pleonard.net" rel="nofollow">Peter Leonard</a></td>
      <td>Visual Culture Computation at the Yale DHLab</td>
    </tr>
    <tr>
      <td>11:25am</td>
      <td>11:45am</td>
      <td>
      <a href="http://douglasduhaime.com" rel="nofollow">Douglas Duhaime</a> (DHLab, Yale)</td>
      <td>Computer Vision and Early Copyright History</td>
    </tr>
    <tr>
      <td>11:45am</td>
      <td>1:15pm</td>
      <td></td>
      <td>Lunch</td>
    </tr>
    <tr>
      <td>1:15pm</td>
      <td>1:35pm</td>
      <td>
      <a href="http://alexwhitebiology.com" rel="nofollow">Alex White</a> (NMNH &amp; Data Science Lab, Smithsonian Insitution)</td>
      <td>Deep Learning, Biogeography, and the Evolution of Plant Shapes</td>
    </tr>
    <tr>
      <td>1:35pm</td>
      <td>1:45pm</td>
      <td>
      <a href="http://www.catherinederose.com" rel="nofollow">Catherine DeRose</a> (DHLab, Yale)</td>
      <td>Neural Style Transfer</td>
    </tr>
    <tr>
      <td>1:45pm</td>
      <td>1:55pm</td>
      <td>
      Adam Metallo (Digitization Program Office, SI)</td>
      <td>Collaboration and Impact</td>
    </tr>
    <tr>
      <td>1:55pm</td>
      <td>2:15pm</td>
      <td>
      <a href="http://www.stephenkrewson.net" rel="nofollow">Stephen Krewson</a> (English, Yale)</td>
      <td>Mining the Medical Heritage Library</td>
    </tr>
    <tr>
      <td>2:15pm</td>
      <td>2:25pm</td>
      <td></td>
      <td>Break</td>
    </tr>
    <tr>
      <td>2:25pm</td>
      <td>2:55pm</td>
      <td>Jacob Kim (Hirshhorn, Smithsonian Institution) and <a href="http://art.yale.edu/DanMichaelson" rel="nofollow">Dan Michaelson</a> (School of Art, Yale)</td>
      <td>Hirshhorn Eye: Activating Artworks using Machine Vision</td>
    </tr>
    <tr>
      <td>2:55pm</td>
      <td>3:25pm</td>
      <td></td>
      <td>Discussion and Next Steps: Ideas for Collaboration</td>
    </tr>
    <tr>
      <td>3:25pm</td>
      <td>3:30pm</td>
      <td>
      <a href="https://provost.yale.edu/who-we-are/susan-gibbons" rel="nofollow">Susan Gibbons</a> (University Librarian &amp; Deputy Provost, Yale)</td>
      <td>Closing Remarks</td>
    </tr>
  </tbody>
</table>  


